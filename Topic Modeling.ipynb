{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from googletrans import Translator\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Response.json of <Response [200]>>\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.musica.com/letras.asp?letra=1305186'\n",
    "page = requests.get(URL)\n",
    "print(page.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_l = []\n",
    "for p in soup.find_all('p'):\n",
    "    p_l.append(p)\n",
    "    \n",
    "raw_song = list(p_l[1])\n",
    "filtered_song = []\n",
    "for j in raw_song:\n",
    "    if ('p***' in j):\n",
    "        filtered_song.append(str(j).replace('p***', 'puta'))\n",
    "        if ('mie***' in j):\n",
    "            filtered_song.append(str(j).replace('mie***', 'mierda'))\n",
    "        else: continue\n",
    "    elif ('mie***' in j):\n",
    "        filtered_song.append(str(j).replace('mie***', 'mierda'))\n",
    "        if ('p***' in j):\n",
    "            filtered_song.append(str(j).replace('p***', 'puta'))\n",
    "        else: continue\n",
    "    else:\n",
    "        filtered_song.append(str(j))  \n",
    "    \n",
    "filtered_song = filtered_song[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    Mi padre es el sol, mi madre la luna',\n",
       " ' mi hermano es el viento y el planeta tierra mi cuna',\n",
       " ' mis unicos hijos son las frases que me invento',\n",
       " ' y mi mayor regalo es vivir este momento',\n",
       " ' en el que siento que callar es un pecado capital']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_song[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "translate_song = []\n",
    "for i in translator.translate(filtered_song):\n",
    "    translate_song.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My father is the sun, the moon my mother',\n",
       " 'my brother is the wind and planet earth my crib',\n",
       " 'my only children are the phrases that invented me',\n",
       " 'and my greatest gift is to live this time',\n",
       " 'in which I feel that silence is a cardinal sin']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_song[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize = word_tokenize(str(translate_song).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "filtered_words = [x for x in word_tokenize if x not in stop_words +list(string.punctuation)+['``',\"''\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(filtered_words):\n",
    "    for char in word:\n",
    "        if char == \"'\":\n",
    "            word = word.replace(\"'\",'')\n",
    "            filtered_words[i] = word\n",
    "        else:\n",
    "            filtered_words[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rap', 10),\n",
       " ('children', 6),\n",
       " ('life', 6),\n",
       " ('know', 5),\n",
       " ('nothing', 5),\n",
       " ('see', 5),\n",
       " ('go', 5),\n",
       " ('want', 4),\n",
       " ('another', 4),\n",
       " ('look', 4),\n",
       " ('like', 4),\n",
       " ('also', 4),\n",
       " ('give', 4),\n",
       " ('although', 4),\n",
       " ('love', 4),\n",
       " ('time', 3),\n",
       " ('feel', 3),\n",
       " ('city', 3),\n",
       " ('science', 3),\n",
       " ('tell', 3)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist([x for x in filtered_words if not x in stop_words]).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_song = pd.DataFrame({'Sentences': translate_song})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(translate_song):\n",
    "    lem = WordNetLemmatizer()\n",
    "    results = []\n",
    "    for token in gensim.utils.simple_preprocess(translate_song):\n",
    "        if token not in list(gensim.parsing.preprocessing.STOPWORDS) +list(string.punctuation):\n",
    "            results.append(lem.lemmatize(token, get_wordnet_pos(token)))      \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_sentences = translate_song['Sentences'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [father, sun, moon, mother]\n",
       "1    [brother, wind, planet, earth, crib]\n",
       "2                 [child, phrase, invent]\n",
       "3               [great, gift, live, time]\n",
       "4          [feel, silence, cardinal, sin]\n",
       "Name: Sentences, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(preprocess_sentences)\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocess_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.246*\"rap\" + 0.111*\"like\" + 0.086*\"time\" + 0.086*\"love\" + 0.086*\"dream\" + 0.073*\"fuck\" + 0.072*\"live\" + 0.070*\"look\" + 0.065*\"tell\" + 0.009*\"child\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.193*\"know\" + 0.163*\"feel\" + 0.129*\"play\" + 0.118*\"come\" + 0.110*\"kid\" + 0.088*\"end\" + 0.014*\"tell\" + 0.014*\"write\" + 0.014*\"time\" + 0.014*\"look\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.190*\"want\" + 0.188*\"life\" + 0.164*\"child\" + 0.105*\"city\" + 0.105*\"science\" + 0.087*\"write\" + 0.011*\"end\" + 0.011*\"rap\" + 0.011*\"tell\" + 0.011*\"look\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ida_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics = 3, id2word=dictionary, passes=100)\n",
    "for idx, topic in ida_model.print_topics(-1): \n",
    "    print('Topic: {} \\nWords: {}\\n'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
